#!/usr/bin/env python3

import theano
from theano import tensor as T
import pandas as pd
from tqdm import trange,tqdm
import numpy as np
from sklearn.preprocessing import normalize
import sklearn.svm
import time
import mnist # pip3 install python-mnist
import os

#theano.config.exception_verbosity="high"
#theano.config.optimizer='None'
theano.config.optimizer='fast_run'
train_filename = 'ionosphere/ionosphere.data'
samples_dir = 'samples'
lr_begin = 1
lr_annealing_T=1
lr=None
n_epochs = 10000
sigma_x = 1.
sigma_z = 1e2#1e4
latent_dim=3
activation_function = T.nnet.sigmoid

class Logger():
    def __init__(self):
        self.filename = "theano_decoder_"+str(time.time())+".log"
        self.f = open(self.filename,'w')

    def __call__(self, *args):
        print(*args, flush=True)
        print(*args,file=self.f, flush=True)

log = Logger()

def calculate_lr(t):
    # decaying learning rate with annealing
    # see: https://www.willamette.edu/~gorr/classes/cs449/momrate.html
    ret = lr_begin / (
        1. + float(t)/lr_annealing_T
    )
    return ret

def shuffle(Z,X,Y):
    sel = np.arange(X.shape[0])
    np.random.shuffle(sel)
    X = X[sel,:]
    Z = Z[sel,:]
    Y = Y[sel,:]
    return Z,X,Y

def fix_data(features,labels):
    X = np.array(features)/255.
    Y = np.expand_dims(np.array(labels),1)
    return X,Y

def load_data():
    log("setting up mnist loader..")
    _mnist = mnist.MNIST(path='./python-mnist/data')
    log("loading training data..")
    X_train,Y_train = fix_data(*_mnist.load_training())
    log("X_train.shape=",X_train.shape,"Y_train.shape=",Y_train.shape)
    log("loading testing data..")
    X_test,Y_test = fix_data(*_mnist.load_testing())
    log("X_test.shape=",X_test.shape,"Y_test.shape=",Y_test.shape)
    return X_train, Y_train, X_test, Y_test

def build_net():
    inputs = T.dvector('inputs')
    W1 = T.dmatrix('W1')
    lin1 = T.dot(W1, inputs)
    lin1.name = 'lin1'
    out1 = activation_function(lin1)
    return inputs, [W1], out1

def update(learnable, grad):
    learnable -= lr * grad

def step(z, x, Ws_vals, grad_fn):
    grad_vals = grad_fn(*([z, x] + Ws_vals))
    Ws_grads = grad_vals[:-1]
    z_grads = grad_vals[-1]
    for curr_W, curr_grad in zip(Ws_vals,Ws_grads):
        update(curr_W, curr_grad)
    #if np.mean(np.abs(z_grads)) > 1e-4:
    #    log(z_grads)
    update(z,z_grads)

def train(Z, X, Ws_vals, grad_fn,repeat=1):
    for z,x in tqdm(list(zip(Z,X))*repeat,desc="training"):
        step(z, x, Ws_vals, grad_fn)

def nll_sum(Z, X, Ws_vals, nll_fn):
    ret = 0
    for z,x in tqdm(list(zip(Z,X)),desc="nll_sum"):
        curr, = nll_fn(*([z, x] + Ws_vals))
        ret += curr
    return ret

def build_negative_log_likelihood(z,outputs,x):
    error_term = 1/sigma_x * T.sum((x-outputs)**2)
    prior_term = 1/sigma_z * T.sum((z)**2)
    nll = error_term + prior_term
    return nll

def test_classifier(Z,Y):
    #classifier = sklearn.svm.SVC()
    log("training classifier..")
    classifier = sklearn.svm.SVC(
        kernel='rbf',
        max_iter=1000
    )
    classifier.fit(Z,Y[:,0])
    log("done. Scoring..")
    svc_score = classifier.score(Z,Y[:,0])
    log("SVC score: %s"%svc_score)

def generate_bunch(epoch,Ws_vals,generate_fn):
    log("generating a bunch of random samples")
    for i in range(10):
        _z = np.random.normal(np.array([0]*latent_dim),sigma_z)
        sample = generate_fn(*([_z]+Ws_vals))
        filename = "%s/sample_epoch_%d_i_%d.npy"%(samples_dir,epoch,i)
        np.save(filename, sample)
    log("done generating.")

def main():
    np.set_printoptions(precision=4, suppress=True)
    try:
        os.mkdir(samples_dir)
    except OSError as e: # directory already exists. It's ok.
        log(e)
    X,Y,X_test,Y_test = load_data()
    x_dim = X.shape[1]
    Z = (np.random.random((X.shape[0],latent_dim))-0.5) * 1
    H1=17# dimensionality of hidden layer
    H2=53# dimensionality of hidden layer
    weights_init = 0.001
    W1_vals = np.random.random((H1,latent_dim))*weights_init
    W2_vals = np.random.random((H2,H1))*weights_init
    W3_vals = np.random.random((x_dim,H2))*weights_init
    Ws_vals = [W1_vals,W2_vals,W3_vals]
    # set up
    z, Ws, outputs = build_net()
    x = T.dvector('x')
    nll = build_negative_log_likelihood(z,outputs,x)
    grads = T.grad(nll,Ws+[z])
    #theano.pp(grad)

    def summary():
        total_nll = nll_sum(Z,X,Ws_vals,nll_fn)
        log("epoch %d"%epoch)
        log("lr %f"%lr)
        log("total nll: {:,}".format(total_nll))
        log("mean Z: %f"%np.mean(Z))
        log("mean abs Z: %f"%np.mean(np.abs(Z)))
        log("std Z: %f"%np.std(Z))
        log("means Ws: %s"%([np.mean(curr) for curr in Ws_vals]))
        log("stds Ws: %s"%([np.std(curr) for curr in Ws_vals]))

    log("compiling theano grad_fn..")
    grad_fn = theano.function([z, x]+Ws, grads)
    log("compiling theano nll_fn..")
    nll_fn = theano.function([z, x]+Ws, [nll])
    log("compiling theano generate_fn..")
    generate_fn = theano.function([z]+Ws, [outputs])
    log("done. epochs loop..")

    # train
    for epoch in range(n_epochs):
        global lr
        lr = calculate_lr(epoch)
        Z,X,Y = shuffle(Z,X,Y)
        summary()
        test_classifier(Z,Y)
        generate_bunch(epoch,Ws_vals,generate_fn)
        train(Z,X,Ws_vals,grad_fn,repeat=1)
        log("saving Z,Y..")
        np.save("theano_decoder_Z.npy",Z)
        np.save("theano_decoder_Y.npy",Y)
        log("done saving.")
    log("epochs loop ended")
    summary()
if __name__=="__main__":
    main()

