\subsection{Learning rate annealing}

Annealing of the learning rate is a technique that uses the progressive reduction
of the learning rate in order to facilitate achieving an optimum of the parameters.

Intuitively one might think that, as the learning progresses, the parameters 
get progressively near the optimum, and, as a consequence, 
the parameter adjustment needs to be of inferior magnitude than the initial one.
This desirable aspect of the learning is achieved by progressively reducing the
learning rate over the epochs.

This is demonstrated by \cite{robbins} which established conditions on
the sequence of learning rates that would ensure reaching an optimum.

The learning rate annealing schema that has been chosen is described by the following
formula \cite{annealing}:

\begin{nalign}
\gamma^{(t)} = \frac{\gamma^{(0)} }{1 + t/T}
\end{nalign}

where $\gamma^{(t)}$ is the learning rate at epoch $t$,
$\gamma^{(0)}$ is the initial learning rate
and $T$ is a hyperparameter whose amount
is the number of epochs it takes for the learning rate
to halve.

The decaying curve is gradual and non-linear, with a long tail.
