\section{Posterior collapse}
\label{posterior_collapse}
\subsection{Diagnosing the collapse of the posterior approximation to the prior distribution}

It was observed\cite{Kingma2017}\cite{1611.02731} (FIXME: cite Bowman et al., 2015 and Kaae SÃ¸nderby et al., 2016 )
that in the initial phases of training, due to weakness of the term $\pxcond$ the term $\kl{\pzonly}{\qzcond}$ 
induces $\qzcond$ to collapse to the prior $\pzonly$.

If the latent variables are independent, then this phenomenon can be diagnosed by looking at the individual Kullback-Leibler divergences
at each latent dimension, as shown in \ref{kl_as_sum} and, for the diagonal-covariance Normal, in \ref{kl_one_d}, \ref{kl_multivariate}.

 The $\kl{\qzcond}{\pzonly}$ term of the $\elbox$, if seen in the context of averaging within a minibatch $\mathcal{M}$, as in
 $\expectxM{\kl{\qzcond}{\pzonly}}$,
 can be interpreted as an approximation to a mutual information term $\mutinf{\boldz}{\boldx}$.
 The implied minimization of the mutual information during optimization of the ELBO forces a high dependence of the $\boldx$ datapoints to the prior $\qzonly$,
 leading to over-regularization of $\qzcond$.

\subsection{KL Annealing}

\cite{Bowman} has done extensive experiments with variational autoencoders
in recurrent neural networks, and points out that it's very likely
that the KL term is much easier to be optimized
and is quickly brought to 0, forcing the $p(\boldz|\boldx)$ term to
collapse to the prior $p(\boldz)$.
He proposes annealing of the KL term to prevent this phenomenon by
lowering the contribution of the term in the initial phases of the learning.

\subsection{Free Bits and Soft Free Bits}
 In order to prevent the collapse of the posterior approximation to the prior, the gradients of the mutual information term can be zeroed by setting a lower-bound
 value to the \emph{nats} expressed from that term, as in:
\begin{nalign}
     \max\left[\lambda,\expectxM{\kl{\qzcond}{\pzonly}}\right]
\end{nalign}

Alternatively, as described in a revision of \cite{1611.02731} \emph{Soft Free Bits}
can be used adapting a $\justkl$ annealing rate $\gamma$ by updating it
at every iteration multiplying it by $1+\epsilon$ or $1-\epsilon$
according to the $\justkl$ being, respectively, larger or lower than $\gamma$.
This is described by the following algorithm:

\begin{algorithm}
\caption{Soft Free Bits}
\begin{algorithmic}[1]

\REQUIRE ~~\\
(1) Initial annealing rate $\gamma$ (to the $\justkl$) \\
(2) $\epsilon$ value to adjust the annealing rate \\
(3) $\lambda$ desired target nats from the $\justkl$
\ENSURE~~\\
(1) The annealing rate $\gamma$ will be adjusted to ease the convergence of the $\justkl$
to the target value $\lambda$

\item[]
\IF{$\justkl > \lambda$}
    \STATE $\gamma \leftarrow \gamma$ * (1 + $\epsilon$)
\ELSE
    \STATE $\gamma \leftarrow \gamma$ * (1 - $\epsilon$)
\ENDIF
\end{algorithmic}
\end{algorithm}




