\section{ELBO as sum of terms dependent on individual datapoints}
\label{elbo_datapoint}

As used by \cite{1312.6114}, the ELBO can be decomposed into
a sum of terms, each dependent only on an individual datapoint. 
This follows the assumption that each datapoint generated by a certain
latent variable realization is independent from both the other datapoints:
\begin{nalign}
\pXcond = \prod_{i=1}^N \pxicondi
\end{nalign}

Same assumption is made on the prior distribution on the latent variables:
\begin{nalign}
\pZ = \prod_{i=1}^N \pzi
\end{nalign}

Hence this is the form for the joint probability:
\begin{nalign}
\pXZ = \pXcond \pZ &= \prod_{i=1}^N \pxicondi \pzi = \prod_{i=1}^N \pxizi
\end{nalign}

For convenience, the chosen form for $\elboX$
will be the \eqref{elbo_crossentropy}.

It's possible to make use of information-theoretical properties
\cite{Bergstrom2008}:

\begin{nalign}
\entropy{\qphiZ} &= \entropy{\qphizone} + \entropy{\qphiZminusone | \qphizone} 
&& \text{chain rule for joint entropy}\\
 &= \entropy{\qphizone} + \entropy{\qphiZminusone}
&& \text{independence of datapoints}\\
&= \sum_i \entropy{\qphizi }&& \text{recursion}
\end{nalign}

Similarly, for $\entropy{\qphiZ,\pZ}$:

\begin{nalign}
\entropy{\qphiZ,\pZ} &= \entropy{\qphizone,\pzone} 
+ \entropy{\qphiZminusone, \pZminusone | \qphizone, \pzone} 
\\
 &= \entropy{\qphizone,\pzone} + \entropy{\qphiZminusone,\pZminusone}
\\
&= \sum_i \entropy{\qphizi,\pzi} 
\end{nalign}

For the third term $\expectqphiZ{\log \pXcond}$:
\begin{nalign}
\expectqphiZ{\log \pXcond} &= \integral{\boldzone}{\cdots \integral{\boldzN}{
    \prod \qphizi \sumiN \log \pxicondi
}\cdots} \\
&= \integral{\boldzone}{\qphizone \cdots \integral{\boldzN}{
     \qphizN \sumiN \log \pxicondi
}\cdots} \\
&= \sumiN \integral{\boldzi}{\qphizi \log \pxicondi}\\
&= \sumiN \expectqphizi{\log \pxicondi}
\end{nalign}

By plugging these forms into the ELBO \eqref{elbo_crossentropy},
it can be shown as a sum of individual objective terms, each of those
is dependent on only a single datapoint:
\begin{nalign}
\elboX = \sumiN -\entropy{\qphizi,\pzi} + \entropy{\qphizi } + \expectqphizi{\log \pxicondi}
\end{nalign}

This sum-based form allows for SGD-like updates; for this reason \cite{1312.6114}
gave \emph{Stochastic Gradient Variational Bayes} as a name for this technique.
