\section{Free bits}

\subsection{Diagnosing the collapse of the posterior approximation to the prior distribution}

It was observed\cite{Kingma2017}\cite{1611.02731} (FIXME: cite Bowman et al., 2015 and Kaae SÃ¸nderby et al., 2016 )
that in the initial phases of training, due to weakness of the term $\pxcond$ the term $\kl{\pzonly}{\qzcond}$ 
induces $\qzcond$ to collapse to the prior $\pzonly$.

If the latent variables are independent, then this phenomenon can be diagnosed by looking at the individual Kullback-Leibler divergences
at each latent dimension, as shown in \ref{kl_as_sum} and, for the diagonal-covariance Normal, in \ref{kl_one_d}, \ref{kl_multivariate}.

 The $\kl{\qzcond}{\pzonly}$ term of the $\elbox$, if seen in the context of averaging within a minibatch $\mathcal{M}$, as in
 $\expectxM{\kl{\qzcond}{\pzonly}}$,
 can be interpreted as an approximation to a mutual information term $\mutinf{\boldz}{\boldx}$.
 The implied minimization of the mutual information during optimization of the ELBO forces a high dependence of the $\boldx$ datapoints to the prior $\qzonly$,
 leading to over-regularization of $\qzcond$.

 In order to prevent the collapse of the posterior approximation to the prior, the gradients of the mutual information term can be zeroed by setting a lower-bound
 value to the \emph{nats} expressed from that term, as in:
\begin{nalign}
     \max\left[\lambda,\expectxM{\kl{\qzcond}{\pzonly}}\right]
\end{nalign}

FIXME: why not try a smooth function instead of max?
