\chapter{Conclusion}

VAERec models introduce a straightforward extension to the AutoRec models.
Probabilistic information on latent variables representing
users or items was exhamined by \cite{vanBaalen2016}.

AutoRec has been chosen as the base model 
for its capability to reconstruct an entire sparse
vector of ratings
belonging to a user, or to an item by estimating all its missing values during 
a single query. 

Our work introduces additional explorations trough the use of variational autoencoders
specifically made to handle sparse input, implemented via parameter masking.

Moreover, posterior approximation improvements have been
added, in the form of \emph{planar flows}\cite{1505.05770} and novel use of 
the more powerful invertible transformation introduced by \emph{RealNVP}\cite{RealNVP}.
Comparisons between different hyperparameter settings have been illustrated.

Overfitting and underfitting were some of the major obstacles in the attempt to 
obtain models with good generization capabilities. In VAE models
these phenomenon can be tackled by altering the coefficient to the $\justkl$
regularizer term. An adaptive method, named \emph{Soft Free Bits} \cite{1611.02731}
has been employed in order to dynamically alter the $\justkl$ coefficient according to
the value of the $\justkl$ term.

The novel use of datapoint comprised of a concatenation of user and item vector indicated
some promising prospects for AutoRec-like models. This input variant
leads to a better fitting than item or user-based models under identical circumstances.
The drawback of overfitting seems to be overcome by 
regularization techniques, in the case of the VaeRec, by careful handling of the
coefficients of the $\justkl$ divergence, with techniques such as \emph{soft free bits}.

\section{Future work}

The field of representation learning and autoencoders is currently object of
growing interest from researchers. Specifically, methods to improve posterior
approximations of VAEs are being researched and could be applied to the base model
VAERec. For instance, of particular interest is \emph{Autoregressive Flow}
\cite{autoregressive_flow}

Specifically to this work, the \emph{RealNVP} transformation 
could be further improved by changing the function $a(\cdot)$ into
a nonlinear function instead of being an affine transformation.

The decoder, or \emph{generator network}, has been chosen as having a spherical
gaussian form with identity covariance matrix $\mathbf{I}$ on $\pxcond$. 
A more informative model
with an arbitrarly-valued diagonal covariance matrix could be employed
in order to give a measure of uncertainty on the estimated ratings.

With an eye on different models, 
\emph{Generative Adversarial Networks} \cite{GAN} seem to be well suited 
for collaborative filtering. The Generator-Discriminator networks might help
obtaining predicted ratings that are as "real" as they could possibly be.

Hyperparameter search for VaeRec models needs to be further
investigated. Specifically, computation-intensive improvements
such as increase in depth and width of the networks should be looked into.
Alternative settings to the $\lambda$ parameter for \emph{soft free bits}
need to be tested in order to find a proper optimum.
Different libraries than \emph{Theano}\cite{theano} should also be tried, as Theano's
development is currently discontinued, in favor of \emph{TensorFlow}\cite{tensorflow}
or \emph{PyTorch}\cite{pytorch}, which might have better support for sparse tensors.
