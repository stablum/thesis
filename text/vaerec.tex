\section{VAERec}

VAERec is one of the contributions of this thesis.
It extends the AutoRec model making use of the VAE framework.

It has been implemented in three variants:

\paragraph{U-VAERec} 
reconstructs user rows by learning the conditional distribution
$\ptheta{\ric|\boldui}$ assumed to be a diagonal-covariance Gaussian
and, jointly, the variational approximation to the posterior
$\qphi{\boldui|\ric}$, also assumed to be a diagonal-covariance Gaussian.

\paragraph{I-VAERec} does the same as \emph{U-VAERec}, 
but with item columns, learning $\ptheta{\rcj|\boldvj}$
and $\qphi{\boldvj|\rcj}$.

\paragraph{UI-VAERec} reconstructs a vector consisting of the concatenation
of a user row and item column.
It learns $\ptheta{\ric,\rcj|\boldz}$
and $\qphi{\boldz|\ric,\rcj}$.
This differs from the MFVA model with 
the MLP decoder proposed by
\cite{vanBaalen2016}, as a distribution on a 
single latent vector $\boldzij$ representing
the user-item pairing is being produced instead 
of having two distinct distributions on
$\boldui$ and $\boldvj$.

\subsection{Sampling the ratings for the \emph{UI} variants}

Training the \emph{UI-VAERec} would require
have very long epochs, as the number of training
datapoints would be the number of ratings.
To prevent problems related to excessive memory usage,
as one rating would be stored as a concatenation of
its user vector $\ric$ and item vector $\rcj$,
epochs have been implemented as random samplings 
of a fixed amount of the ratings.
The same sampling is performed in the test set.
\subsection{Masking the Adam optimizer}

Given its properties, Adam \cite{KingmaB14} has been choosen
as optimization algorithm.

The VAERec, similarly to the AutoRec, needs to be selective on
which parameters needs to be updated: both in the first
layer of the encoder
and the last layer of the decoder, only the weights
that are connected to existing ratings can be updated.

Provided a binary mask $\mask$ of a parameters tensor $\boldsymbol\theta$
then Adam has the two assignments of $\widehat{\boldm}_t$
and $\widehat{\boldv}_t$ modified from the original algorithm as follows:

\begin{nalign}
\widehat{\boldm}_t &\leftarrow \frac{\boldm_t}{1 - \beta^t_1} \odot \mask
+ \boldm_{t-1}\odot (1-\mask)\\
\widehat{\boldv}_t &\leftarrow \frac{\boldv_t}{1-\beta^t_2}\odot\mask
+ \boldv_{t-1}\odot (1-\mask)
\end{nalign}
