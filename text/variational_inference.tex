\section{Variational inference}

Bayesian inference is concerned on updating an existing hypothesis on a statistical model
on a data source, with data samples empirically obtained from that data source.

In other words, an existing model hypothesis is called a \emph{prior distribution} $p(\model)$; 
the probability of the samples $\dataset$ under the model $\model$ is called the \emph{likelihood} $p(\model|\dataset)$. The usually not available true probability of the samples $\dataset$ is called \emph{evidence} $p(\dataset)$.

By using bayes theorem it's possible to obtain the \emph{posterior distribution} $p(\model|\dataset)$ of the model $\model$ after observation of the data $\dataset$:

\begin{nalign}
p(\model|\dataset) = \frac{p(\dataset|\model)p(\model)}{p(\dataset)}
\end{nalign}

In \emph{representation learning} it's assumed that each datapoint $\boldx$
are generated by unknown (latent) variables $\boldz$. Hence the problem
of finding the generative model of the data becomes
a problem of modeling the distribution of the latent variables.
The initial hypothesis on how the latent variables are distributed $p_\theta(\boldz)$
is updated with the observed datapoint $p_\theta(\boldx|\boldz)$
within the framework of a generative model represented by $\theta$
which describes how $\boldx$ relates to a certain latent variable assignment $\boldz$.
In this new context the bayesian rule is used to infer the posterior distribution
of an arbitrary setting of the latent variables $\boldz$:

\begin{nalign}
p_\theta(\boldz|\boldx) = \frac{p_\theta(\boldx|\boldz)p_\theta(\boldx)}{p_\theta(\boldx)}
\end{nalign}

As the true posterior $p_\theta(\boldz|\boldx)$ is typically unavailable,
an approximation $q(\boldz|\boldx)$ is looked for
via \emph{variational inference} methods.
Variational inference is concerned to
minimize the distance between the approximation and the true posterior\cite{Fox2012},
which is typically done by minimizing the \emph{Kullback-Leibler distance}
$\kl{q(\boldz|\boldx)}{p_\theta(\boldz|\boldx)}$.

The $\justkl$ can be decomposed  into:

\begin{nalign}
\kl{q(\boldz|\boldx)}{p_\theta(\boldz|\boldx)}
&= 
\expect{q(\boldz|\boldx)}{
    \log\frac{
        q(\boldz|\boldx) 
    }{
        p_\theta(\boldx,\boldz)
    }
}
+ \log p_\theta(\boldx)
\end{nalign}

We can use the shorthand $\elbox = - 
\expect{q(\boldz|\boldx)}{
    \log\frac{
        q(\boldz|\boldx) 
    }{
        p_\theta(\boldx,\boldz)
    }
}
$ ; by noticing that $\log p_\theta(\boldx)$
is a fixed quantity w.r.t. $\boldz$,
and that $\justkl$ quantities are
always non-negative,
it's easy to see how $\elbox$
is a lower bound to $p_\theta(\boldx)$
and the maximization of $\elbox$
implies necessarily the minimization of 
$\kl{q(\boldz|\boldx)}{p_\theta(\boldz|\boldx)}$
.

This is the basis of variational inference, and 
you can refer to Appendix \ref{elbo},  
\ref{elbo_datapoint}
and
\ref{r_elbo}
for details on the derivations of the lower bound.

