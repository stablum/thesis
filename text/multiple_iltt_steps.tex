\section{Multiple nested transformation steps}\label{multiple_iltt_steps}

\begin{figure}
\caption{Average squared error of a VAE with Planar Flows trained
         on the MNIST dataset. Parameters are: learning rate $5*10^{-7}$,
         activation functon sigmoid, latent dimensionality 16,
         size of the hidden layer 64. The three experiments are differentiated
         by the NF's K being 4, 5 and 6. The plot demonstrates that,
         as K gets larger, the reconstruction error deminishes over the 1300 epochs.
         The optimizer used is Adam.}
\centering
\includegraphics[width=13cm]{mnist_nfk456.eps}
\end{figure}

A transformation $\tr(\boldz)$ might be composed
of multiple nested but similar transformation steps
each with it's own parameters:

\begin{nalign}
\tr(\boldz) &= t_k \circ t_{k-1} \circ \ldots \circ t_1(\boldzzero)
\end{nalign}

By using the chain rule, the gradient of the transformation becomes:
\begin{nalign}
\Dtr{\boldzzero}
&= \prodk{\derivtk{\boldzkminusone}}
\end{nalign}

The determinant of the product of square matrices is the product of their determinants, hence:
\begin{nalign}
\det \Dtr{\boldzzero} &= \prodk{\det \derivtk{\boldzkminusone}}
\end{nalign}

The expectation term, as previously illustrated, can be approximated with
a single Monte Carlo sample:

\begin{nalign}
\expectqzero{\log \left( \abs{\Dtr{\boldzkminusone}} \right)}
&\approx \log \abs{\prodk{\det \derivtk{\boldzkminusone}}}
\\
&=\sumk{ \log \abs{\derivtk{\boldzkminusone}}}
\end{nalign}
\subsection{Implementation with Planar Transformations}

With planar transformations, the derivative becomes:

\begin{nalign}
\Dtr{\boldzzero}
&=\prodk{ \identity + \bolduk h^\prime(\wtk \boldzkminusone + \bk)\wtk }
\end{nalign}

The expectation sample becomes:

\begin{nalign}
\expectqzero{\log \left( \abs{\Dtr{\boldzkminusone}} \right)}
&\approx
\sumk{ \log \abs{1 + h^\prime(\wtk \boldzkminusone + \bk)\wtk \bolduk }}
\end{nalign}

