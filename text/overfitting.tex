\section{Dealing with overfitting}

One of the major issues in some AutoRec/VaeRec models
is overfitting. 
Notwithstanding the fact that VAE models are plagued by
over-regularization caused by the posterior approximation collapse described in section
\ref{posterior_collapse}, under specific circumstances overfitting is prominent,
as in using \emph{UI} (user+item) datapoint schemas.
In these cases, the model performs very well on the training
set but unsatisfactory on the test set.

\paragraph{Dropout on the input layer}
Dropout \cite{Srivastava2014}
is a technique aimed at preventing overfitting
which employs randomly dropping units
and their connections
during training.
This would ensure to obtain a neural network
which can function even when parts are deactivated.
Moreover, it would prevent that a single unit becomes
entirely representative of a certain aspect of the training data
data.

It has been observed that applying Dropout on just the input layer
of the VaeRec models,
overfitting can be prevented to a certain degree,
possibly by a similar way of action as denoising autoencoders.
\cite{Vincent2010}.

\paragraph{Narrowing the bottleneck}
This is a common technique, typically used with regular autoencoders.
Information is being channeled trough a limited number of nodes,
forcing the neural network to lose hopefully unrelevant information
which might be dataset noise.
\paragraph{Regularization}
Regularization has been tested with either L1 or L2 norms,
without significant improvements.
\paragraph{Increasing the depth}
Depth has been studied in \cite{MhaskarLP17},\cite{Poggio2015}
as a method to improve representations. Better representations usually lead
to a better identification and disposition of overfitting noise. 
