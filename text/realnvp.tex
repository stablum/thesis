\section{RealNVP Transformations}
\label{realnvp}

\cite{RealNVP} introduced a very simple invertible function of the form:

\begin{nalign}
\left\{ 
    \begin{array}{ll}
    t(\boldz)_{1:d} &= \boldz_{1:d}
    \\
    t(\boldz)_{d+1:K} &= \boldz_{d+1:K}\odot \exp\left(s(\boldz_{1:d})\right) + a(\boldz_{1:d})
    \end{array}
\right.
\end{nalign}

    The inverse can be trivially obtained as:

\begin{nalign}
\left\{
    \begin{array}{ll}
    \boldz_{1:d} & = t(\boldz)_{1:d}\\
    \boldz_{d+1:K} &= \left( t(\boldz)_{1:d} - a(\boldz_{1:d}) \right) \underbrace{\oslash \exp(s(t(\boldz_{1:d})))}_{\odot \exp(-s(t(\boldz_{1:d})))}
    \end{array}
\right.
\end{nalign}

$s(\cdot)$ can be any dimensionality-preserving nonlinear function, such as a neural network with nonlinear activations. $a(\cdot)$ is an affine transformation.
In this work's implementation $d$ is set $d = K/2$. 

The main advantage of using such transformations is that the Jacobian matrix is triangular,
hence its determinant is obtained 
from the diagonal, culminating with the form
    $\exp\left(\sum_j s(\boldz_{1:d})_j \right)$ 

