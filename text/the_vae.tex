\section{The Variational Auto-Encoder}

\cite{1312.6114} introduced a model aimed at posterior inference 
on datasets with high-dimensional datapoints.

The model is based on a \emph{generator network} which outputs a conditional
distribution $\pxcond$ 
in datapoint-space given a realization of the latent variables $\boldz$.

The posterior distribution $\pzcond=\integral{\boldz}{\pxcond \pthetaz}$ is intractable,
hence an approximating \emph{recognition network}
$\qzcond$ is introduced whose parameters $\phi$ are 
optimized via variational inference. 
The optimization of $\phi$ happens simultaneously with the parameters $\theta$.

It was also shown experimentally how a Monte Carlo approximation of
the ELBO (section \ref{elbo_datapoint})
by sampling the posterior approximation is sufficient to
achieve good learning performances.

Moreover, \cite{1312.6114} experimentally demonstrated how just a single Monte Carlo samples might achieve good approximation.

Since values of $\boldz$ are being sampled, this would prevent gradients from flowing
in a backpropagation-like way. To circumvent this problem, a \emph{reparameterization trick}
has been employed by using a sample $\epsilon$ which is always drawn from
a $\mathcal{N}(\mathbf{0},\mathbf{I})$ Normal distribution.
By using the transformation:
\begin{nalign}
\hat{\boldz} = \boldmu_{\phi} + \boldsigma_{\phi} \cdot \epsilon
\end{nalign}
a sample is obtained from the distribution $\mathcal{N}(\boldmu_{\phi},\boldsigma_{\phi})$.

The sum-based form that allows for SGD-like updates described in section
\ref{elbo_datapoint}
and
the fact that a Monte Carlo approximation is used for the approximation of one datapoint term
are the reason that \cite{1312.6114}
gave \emph{Stochastic Gradient Variational Bayes} as a name for this technique.
