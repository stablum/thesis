\section{The Variational Auto-Encoder}

\cite{1312.6114} introduced a model aimed at posterior inference 
on datasets with high-dimensional datapoints.

The model is based on a \emph{generator network} which outputs a conditional
distribution $\pxcond$ 
in datapoint-space given a realization of the latent variables $\boldz$.

The posterior distribution $\pzcond=\integral{\boldz}{\pxcond \pthetaz}$ is intractable,
hence an approximating \emph{recognition network}
$\qzcond$ is introduced whose parameters $\phi$ are
optimized via variational inference.

It was also shown experimentally how a Monte Carlo approximation of
the single-datapoint ELBO (section \ref{elbo_datapoint})
using a single sample of the latent variable is sufficient to
achieve good learning performances. (FIXME: "doubly stochastic (eventually cite paper)??")

The sum-based form that allows for SGD-like updates described in section
\ref{elbo_datapoint}
and
the single sample used for the approximation of one datapoint term
are the reason that \cite{1312.6114}
gave \emph{Stochastic Gradient Variational Bayes} as a name for this technique.
